## The Core Problem

We have a file `xy_data.csv` with 1500 (x, y) points. We also have a set of parametric equations:

$$
x = \left( t \cdot \cos(\theta) - e^{M|t|} \cdot \sin(0.3t) \cdot \sin(\theta) + X \right)
$$
$$
y = \left( 42 + t \cdot \sin(\theta) + e^{M|t|} \cdot \sin(0.3t) \cdot \cos(\theta) \right)
$$

The goal is to find the three numbers—**$\theta$ (theta)**, **M**, and **X**—that make the curve generated by these equations (for **t in [-6, 60]**) pass as close as possible to all 1500 data points.

---

## Running the file

```bash
# 0. cd into file - 'Technical_Assessment'
cd 'Technical_Assessment'

# 1. Create a virtual environment
python3 -m venv venv

# 2. Activate it
source venv/bin/activate

# 3. Install packages
pip install -r requirements.txt

# 4. Run training
python3 run_training.py
```

When it's done, it will print the final best parameters and a LaTeX string to the console. It will also save a `loss_curves.png` plot in the root directory.

---

## Summary of the process and baseline

* **Setup:** The `main()` function starts. It calls `utils.setup_logging()` to make the console output look nice and `utils.set_seed(42)` to make sure the run is reproducible.

* **Load Data:** It calls `data_loader.load_data()` to read `xy_data.csv` into a pandas DataFrame.

* **Preprocess:** It calls `data_loader.preprocess_data()` to:
    * Convert the `x` and `y` columns to PyTorch tensors (`x_data`, `y_data`).
    * Create the 100-point `t_vec` tensor (from -6 to 60).
    * Move all three tensors to the correct device (e.g., `'cuda'` or `'cpu'`).

* **Start Multi-Run Loop:** The script begins a `for` loop that runs `config.N_RUNS` times [ 100 times, Tested with 10, 25 and 120 and finalized n\_runs = 100].

* **Inside Each Run:**
    * **Initialize:** A `ParametricModel` is created (with new random initial values for $\theta$, M, X) and a new `Adam` optimizer is set up.
    * **Training Loop:** A second, inner `for` loop starts, running for `config.N_EPOCHS` [100\*8000 epochs] steps.
    * **Forward Pass:** `model(t_vec)` is called. This uses the current $\theta$, M, X into the equations to generate a 100-point predicted curve (`x_pred`, `y_pred`).
    * **Loss Calc:** `l1_loss(x_pred, y_pred, x_data, y_data)` is called. This calculates the total distance from our 1500 true points to the 100-point predicted curve.
    * **Backward Pass:** `loss.backward()` is called. PyTorch's autograd automatically computes the gradients.
    * **Optimize:** `optimizer.step()` updates the parameters based on the gradients. `scheduler.step()` adjusts the learning rate.
    * **Clamp:** `model.clamp_parameters()` is called to force the parameters to stay within their predefined bounds.

* **End of Run:** After 100\*8000 epochs, the run is finished. The script stores the best loss and parameters from that specific run.

> {I tried to speed it up with assigning the CPU threads to each run and tried to parallelize, but didnt find signifcant time improvement.}

* **Find Best:** After all 100 runs are complete, the script looks at the 100 final results and finds the one that had the overall lowest loss.

* **Report & Plot:**
    * The `plot_loss_curves()` function is called to create `loss_curves.png`.
    * The final, best parameters (**$\theta$**, **M**, **X**) and the best loss are logged to the console, and a formatted LaTeX string is printed.